{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper this is based attempting to replicate: https://arxiv.org/pdf/2402.01785\n",
    "\n",
    "Useful DoubleML docs: https://docs.doubleml.org/stable/guide/guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "import doubleml\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import warnings\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import clone\n",
    "\n",
    "from doubleml import DoubleMLData\n",
    "from doubleml import DoubleMLPLR\n",
    "from doubleml.datasets import make_plr_CCDDHNR2018\n",
    "\n",
    "face_colors = sns.color_palette('pastel')\n",
    "edge_colors = sns.color_palette('dark')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"init complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models \n",
    "#### Baseline Model\n",
    "Uses `LightGBM` package _only_ to estimate nuisance elements on _only_ the tabular data \n",
    "\n",
    "#### Deep Model\n",
    "Implemented exactly as in _Figure 2_ in paper\n",
    "- For text, they use a `RoBERTa` Model pretrained on a `Twitter` Dataset\n",
    "- For images, they use a `VIT` Model pretrained on the `ImageNet-21k` Dataset\n",
    "- For tabular data, they use a `SAINT` model implemented in `pytorch-widedeep`\n",
    "\n",
    "#### Embedding Model \n",
    "Builds on the `Deep Model`, but instead of passing embeddings directly through fusion head/predictive workflow, passes general embedding $H_e$ and data $X_{tab}$ to a boosting algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets (change labelling to be \\textbf)\n",
    "### Tabular\n",
    "`DIAMONDS` dataset, downsampled to create dataset with $N=50,000$ \n",
    "- $\\tilde{X}_{tab}$ is the logarithm of the price\n",
    "- $X_{tab}$ is everything else\n",
    "### Image\n",
    "`CIFAR-10` dataset, specifically the training set ($N=50,000$), which is 32x32 colour images in 10 different classes\n",
    " - $\\tilde{X}_{img}$ is a numerical representation of the label\n",
    " - $X_{img}$ is the image itself\n",
    "### Text\n",
    "`IMDB` dataset, both the training and test samples\n",
    "- $\\tilde{X}_{txt}$ is the binary (positive/negative) sentiment label\n",
    "- $X_{txt}$ is the review itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jn446\\AppData\\Local\\Temp\\ipykernel_20796\\501465470.py:69: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_mod_tild = np.array([txt_tild_df, tab_tild_df, img_tild_df])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>review</th>\n",
       "      <th>imgs</th>\n",
       "      <th>D</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=32x32 at ...</td>\n",
       "      <td>3.094251</td>\n",
       "      <td>-0.949851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=32x32 at ...</td>\n",
       "      <td>1.361744</td>\n",
       "      <td>-0.560637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=32x32 at ...</td>\n",
       "      <td>1.518297</td>\n",
       "      <td>-0.234724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.522928</td>\n",
       "      <td>-4.178922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.820808</td>\n",
       "      <td>-1.549256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat  cut  color  clarity  depth  table     x     y     z  \\\n",
       "0   0.23    5      2        1   61.5   55.0  3.95  3.98  2.43   \n",
       "1   0.21    4      2        2   59.8   61.0  3.89  3.84  2.31   \n",
       "2   0.23    2      2        4   56.9   65.0  4.05  4.07  2.31   \n",
       "3   0.29    4      6        3   62.4   58.0  4.20  4.23  2.63   \n",
       "4   0.31    2      7        1   63.3   58.0  4.34  4.35  2.75   \n",
       "\n",
       "                                              review  \\\n",
       "0  One of the other reviewers has mentioned that ...   \n",
       "1  A wonderful little production. <br /><br />The...   \n",
       "2  I thought this was a wonderful way to spend ti...   \n",
       "3  Basically there's a family where a little boy ...   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
       "\n",
       "                                                imgs         D         Y  \n",
       "0  <PIL.Image.Image image mode=RGB size=32x32 at ...  3.094251 -0.949851  \n",
       "1  <PIL.Image.Image image mode=RGB size=32x32 at ...  1.361744 -0.560637  \n",
       "2  <PIL.Image.Image image mode=RGB size=32x32 at ...  1.518297 -0.234724  \n",
       "3                                                NaN  2.522928 -4.178922  \n",
       "4                                                NaN  2.820808 -1.549256  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#process real data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "tab_df = pd.read_csv(\"diamonds.csv\")\n",
    "\n",
    "cut_di = {'Ideal':5, 'Premium':4, 'Good':2, 'Very Good':3, 'Fair':1}\n",
    "tab_df['cut'].replace(cut_di,inplace=True)\n",
    "col_di={'E': 2, 'I': 6, 'J': 7, 'H': 5, 'F': 3, 'G': 4, 'D': 1}\n",
    "tab_df['color'].replace(col_di,inplace=True)\n",
    "clar_di={'SI2': 1, 'SI1': 2, 'VS1': 4, 'VS2': 3, 'VVS2': 5, 'VVS1': 6, 'I1': 0, 'IF': 7}\n",
    "tab_df['clarity'].replace(clar_di,inplace=True)\n",
    "\n",
    "tab_tild_df=np.log(tab_df['price']).to_numpy()\n",
    "tab_df=tab_df.drop(columns=['price','Unnamed: 0'])\n",
    "\n",
    "\n",
    "txt_df=pd.read_csv(\"IMDB Dataset.csv\")\n",
    "\n",
    "sent_di={'positive':1,'negative':0}\n",
    "txt_df['sentiment'].replace(sent_di,inplace=True)\n",
    "\n",
    "txt_tild_df = txt_df['sentiment'].to_numpy()\n",
    "txt_df=txt_df['review']\n",
    "\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file,'rb') as fo:\n",
    "        dict=pickle.load(fo,encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "\n",
    "img_dict=unpickle(r'cifar-10-batches-py\\data_batch_1')\n",
    "labels=img_dict[b'labels']\n",
    "names=img_dict[b'filenames']\n",
    "imgs=img_dict[b'data']\n",
    "\n",
    "img_tild_df=np.array(labels)\n",
    "\n",
    "img_df=imgs\n",
    "\n",
    "def CIFAR2img(img):\n",
    "    red = np.split(img, 32*3)[0:32]\n",
    "    green = np.split(img, 32*3)[32:64]\n",
    "    blue = np.split(img, 32*3)[64:96]\n",
    "    \n",
    "    return Image.fromarray(np.dstack((red,green,blue)), \"RGB\")\n",
    "\n",
    "imgs=list(map(CIFAR2img, imgs[0:3]))\n",
    "img_df = pd.DataFrame(imgs, columns=['imgs'])\n",
    "\n",
    "\n",
    "\n",
    "# DEVICE = torch.device('cpu')\n",
    "# pipe = pipeline(task=\"image-feature-extraction\", model_name=\"google/vit-base-patch16-384\", device=DEVICE, pool=True)\n",
    "# output = pipe(imgs, return_tensors=True)\n",
    "# img_embeddings=torch.stack(output).squeeze()\n",
    "#NOTE: above is important for image pipeline\n",
    "\n",
    "X_mod_tild = np.array([txt_tild_df, tab_tild_df, img_tild_df])\n",
    "N=min([np.shape(X)[0] for X in X_mod_tild])\n",
    "X_mod_tild = np.array([txt_tild_df[0:N], tab_tild_df[0:N], img_tild_df[0:N]])\n",
    "\n",
    "\n",
    "#contruct synthetic data\n",
    "theta0=0.5\n",
    "\n",
    "g0_tild=np.sum(np.array(list(map(lambda X: (X - np.full(N,np.mean(X)))/np.std(X) ,X_mod_tild))),axis=0)\n",
    "\n",
    "m0_tild= -g0_tild\n",
    "\n",
    "np.random.seed(20)\n",
    "D=m0_tild + np.random.normal(0,1,size=N)\n",
    "Y=theta0*D + g0_tild + np.random.normal(0,1,size=N)\n",
    "D_df=pd.DataFrame(D, columns=['D'])\n",
    "Y_df=pd.DataFrame(Y, columns=['Y'])\n",
    "\n",
    "X_mod=tab_df[:N].join(other=[txt_df[:N], img_df[:N]])\n",
    "df=X_mod.join(other=[D_df,Y_df])\n",
    "df.head()\n",
    "#NOTE: need to double check with full 10000/50000 batches \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up architecture/model: Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deep/embedding common classes\n",
    "\n",
    "\n",
    "# embedding_extractor = pipeline(model=\"google-bert/bert-base-cased\", task=\"feature-extraction\", device=0)\n",
    "# result = embedding_extractor(\"This is a simple test.\", return_tensors=True)\n",
    "\n",
    "txt_model = \"FacebookAI/roberta-base\"\n",
    "txt_embedding_extractor = pipeline(model=txt_model, task=\"feature-extraction\", device=0 if torch.cuda.is_available() else -1)\n",
    "txt_embeddings= txt_embedding_extractor(all_text_tensor, return_tensors=True)\n",
    "\n",
    "img_model=\"VIT\"\n",
    "\n",
    "tab_model = \"SAINT\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MMEmbeddingNetwork(nn.Module):\n",
    "    def __init__(self, img_embed_size, txt_embed_size,HE_size,hiddenSize=100):\n",
    "        super(self).__init__()\n",
    "        # self.inp = nn.Linear(imgEmbSize+txtEmbSize, hiddenSize)\n",
    "        # self.fc = nn.Linear(hiddenSize,hiddenSize)\n",
    "        # self.out = nn.Linear(hiddenSize,H_ESize)\n",
    "        self.fc=nn.Linear(img_embed_size+txt_embed_size, HE_size)\n",
    "    def forward(self,comb_embed):\n",
    "        # x=combinedEmbed\n",
    "        # x=self.inp(x)\n",
    "        # x=self.fc(x)\n",
    "        # return self.out(x)\n",
    "        x=self.fc(comb_embed)\n",
    "        return activF(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up architecture/model: Deep\n",
    "    \n",
    "\n",
    "class PredictiveUnit(nn.Module):\n",
    "    def __init__(self,gen_embed_size):\n",
    "        super(self).__init__()\n",
    "        self.fhead_outcome = nn.Linear(gen_embed_size,1)\n",
    "        self.fhead_treat = nn.Linear(gen_embed_size,1)\n",
    "    \n",
    "    def forward(self, gen_embed):\n",
    "        l_hat=self.fhead_outcome(gen_embed)\n",
    "        m_hat=self.fhead_treat(gen_embed)\n",
    "        return m_hat,l_hat\n",
    "    \n",
    "\n",
    "class DeepModel(nn.Module):\n",
    "    def __init__(self, txt_pipeline, img_pipeline, tab_pipeline, img_embed_size, txt_embed_size,HE_size,gen_embed_size):\n",
    "        super(self).__init__()\n",
    "        self.txt_in,self.img_in,self.tab_in=txt_pipeline, img_pipeline, tab_pipeline\n",
    "        self.multimod = MMEmbeddingNetwork(img_embed_size, txt_embed_size,HE_size)\n",
    "        self.pred=PredictiveUnit(gen_embed_size)\n",
    "\n",
    "    def forward(self, txt, img, tab):\n",
    "        txt_embed=self.txt_in(txt)\n",
    "        img_embed = self.img_in(img)\n",
    "        tab_embed = self.tab_in(tab)\n",
    "        comb_embed = txt_embed + img_embed \n",
    "        H_E = self.multimod(comb_embed)\n",
    "        G_E = H_E + tab_embed\n",
    "        return self.pred(G_E)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up architecture/model: Embedding\n",
    "\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, boost_alg,txt_pipeline, img_pipeline, img_embed_size, txt_embed_size,HE_size):\n",
    "        self.txt_in,self.img_in=txt_pipeline, img_pipeline\n",
    "        self.multimod = MMEmbeddingNetwork(img_embed_size, txt_embed_size,HE_size)\n",
    "        self.boosting_alg= boost_alg\n",
    "    def forward(self, txt, img, tab):\n",
    "        txt_embed=self.txt_in(txt)\n",
    "        img_embed = self.img_in(img)\n",
    "        comb_embed = txt_embed + img_embed \n",
    "        H_E = self.multimod(comb_embed)\n",
    "        return self.boosting_alg(H_E, tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML training loop: Deep\n",
    "def DeepLoss(D,Y,m_hat,l_hat):\n",
    "    \"\"\"can handle 1-D vectors\"\"\"\n",
    "    D_rms_err=torch.sqrt(torch.sum(torch.square(D-m_hat)))\n",
    "    Y_rms_err=torch.sqrt(torch.sum(torch.square(Y-l_hat)))\n",
    "    return D_rms_err*Y_rms_err\n",
    "\n",
    "deepnet=DeepModel(txt_pipeline, img_pipeline, tab_pipeline, img_embed_size, txt_embed_size,HE_size,gen_embed_size)\n",
    "\n",
    "def train(deepnet,n_epochs=1000, batch_size=100,loss_fn=DeepLoss):\n",
    "    \"\"\"Training params need work\n",
    "    form depends on pd.DataFrame/torch.tensor implementation details\"\"\"\n",
    "    optimizer=torch.optim.Adam(deepnet.parameters(),lr=0.001)\n",
    "    for i_epoch in tqdm(range(n_epochs)):\n",
    "        \n",
    "        #NOTE: it's about to get spicy here!\n",
    "        \n",
    "        m_hat,l_hat = deepnet(txt, img, tab)\n",
    "        loss = loss_fn(D,Y,m_hat,l_hat)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Finished {i_epoch+1}/n_epoch, loss = {loss}\", end = '\\r')\n",
    "\n",
    "train(deepnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML training loop: Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common DoubleML pass-through implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save and export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import model (if needed (how to structure selective cell runs?))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting/performance analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
