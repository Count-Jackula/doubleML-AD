{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper this is based attempting to replicate: https://arxiv.org/pdf/2402.01785\n",
    "\n",
    "Useful DoubleML docs: https://docs.doubleml.org/stable/guide/guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "import doubleml\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import warnings\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import clone\n",
    "\n",
    "from doubleml import DoubleMLData\n",
    "from doubleml import DoubleMLPLR\n",
    "from doubleml.datasets import make_plr_CCDDHNR2018\n",
    "\n",
    "face_colors = sns.color_palette('pastel')\n",
    "edge_colors = sns.color_palette('dark')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"init complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models \n",
    "#### Baseline Model\n",
    "Uses `LightGBM` package _only_ to estimate nuisance elements on _only_ the tabular data \n",
    "\n",
    "#### Deep Model\n",
    "Implemented exactly as in _Figure 2_ in paper\n",
    "- For text, they use a `RoBERTa` Model pretrained on a `Twitter` Dataset\n",
    "- For images, they use a `VIT` Model pretrained on the `ImageNet-21k` Dataset\n",
    "- For tabular data, they use a `SAINT` model implemented in `pytorch-widedeep`\n",
    "\n",
    "#### Embedding Model \n",
    "Builds on the `Deep Model`, but instead of passing embeddings directly through fusion head/predictive workflow, passes general embedding $H_e$ and data $X_{tab}$ to a boosting algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets (change labelling to be \\textbf)\n",
    "### Tabular\n",
    "`DIAMONDS` dataset, downsampled to create dataset with $N=50,000$ \n",
    "- $\\tilde{X}_{tab}$ is the logarithm of the price\n",
    "- $X_{tab}$ is everything else\n",
    "### Image\n",
    "`CIFAR-10` dataset, specifically the training set ($N=50,000$), which is 32x32 colour images in 10 different classes\n",
    " - $\\tilde{X}_{img}$ is a numerical representation of the label\n",
    " - $X_{img}$ is the image itself\n",
    "### Text\n",
    "`IMDB` dataset, both the training and test samples\n",
    "- $\\tilde{X}_{txt}$ is the binary (positive/negative) sentiment label\n",
    "- $X_{txt}$ is the review itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jn446\\AppData\\Local\\Temp\\ipykernel_20796\\3170644937.py:69: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_mod_tild = np.array([txt_tild_df, tab_tild_df, img_tild_df])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>review</th>\n",
       "      <th>imgs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>63.5</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.38</td>\n",
       "      <td>6.31</td>\n",
       "      <td>4.03</td>\n",
       "      <td>Fun, entertaining movie about WWII German spy ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>61.4</td>\n",
       "      <td>61.0</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.36</td>\n",
       "      <td>3.92</td>\n",
       "      <td>Give me a break. How can anyone say that this ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>61.1</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.48</td>\n",
       "      <td>6.44</td>\n",
       "      <td>3.95</td>\n",
       "      <td>This movie is a bad movie. But after watching ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>62.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.29</td>\n",
       "      <td>3.94</td>\n",
       "      <td>This is a movie that was probably made to ente...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>64.9</td>\n",
       "      <td>59.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>6.13</td>\n",
       "      <td>4.00</td>\n",
       "      <td>Smashing film about film-making. Shows the int...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      carat  cut  color  clarity  depth  table     x     y     z  \\\n",
       "9995    1.0    3      2        1   63.5   56.0  6.38  6.31  4.03   \n",
       "9996    1.0    4      2        1   61.4   61.0  6.41  6.36  3.92   \n",
       "9997    1.0    4      2        1   61.1   58.0  6.48  6.44  3.95   \n",
       "9998    1.0    4      1        2   62.0   58.0  6.41  6.29  3.94   \n",
       "9999    1.0    1      1        2   64.9   59.0  6.20  6.13  4.00   \n",
       "\n",
       "                                                 review imgs  \n",
       "9995  Fun, entertaining movie about WWII German spy ...  NaN  \n",
       "9996  Give me a break. How can anyone say that this ...  NaN  \n",
       "9997  This movie is a bad movie. But after watching ...  NaN  \n",
       "9998  This is a movie that was probably made to ente...  NaN  \n",
       "9999  Smashing film about film-making. Shows the int...  NaN  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#process real data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "tab_df = pd.read_csv(\"diamonds.csv\")\n",
    "\n",
    "cut_di = {'Ideal':5, 'Premium':4, 'Good':2, 'Very Good':3, 'Fair':1}\n",
    "tab_df['cut'].replace(cut_di,inplace=True)\n",
    "col_di={'E': 2, 'I': 6, 'J': 7, 'H': 5, 'F': 3, 'G': 4, 'D': 1}\n",
    "tab_df['color'].replace(col_di,inplace=True)\n",
    "clar_di={'SI2': 1, 'SI1': 2, 'VS1': 4, 'VS2': 3, 'VVS2': 5, 'VVS1': 6, 'I1': 0, 'IF': 7}\n",
    "tab_df['clarity'].replace(clar_di,inplace=True)\n",
    "\n",
    "tab_tild_df=np.log(tab_df['price']).to_numpy()\n",
    "tab_df=tab_df.drop(columns=['price','Unnamed: 0'])\n",
    "\n",
    "\n",
    "txt_df=pd.read_csv(\"IMDB Dataset.csv\")\n",
    "\n",
    "sent_di={'positive':1,'negative':0}\n",
    "txt_df['sentiment'].replace(sent_di,inplace=True)\n",
    "\n",
    "txt_tild_df = txt_df['sentiment'].to_numpy()\n",
    "txt_df=txt_df['review']\n",
    "\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file,'rb') as fo:\n",
    "        dict=pickle.load(fo,encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "\n",
    "img_dict=unpickle(r'cifar-10-batches-py\\data_batch_1')\n",
    "labels=img_dict[b'labels']\n",
    "names=img_dict[b'filenames']\n",
    "imgs=img_dict[b'data']\n",
    "\n",
    "img_tild_df=np.array(labels)\n",
    "\n",
    "img_df=imgs\n",
    "\n",
    "def CIFAR2img(img):\n",
    "    red = np.split(img, 32*3)[0:32]\n",
    "    green = np.split(img, 32*3)[32:64]\n",
    "    blue = np.split(img, 32*3)[64:96]\n",
    "    \n",
    "    return Image.fromarray(np.dstack((red,green,blue)), \"RGB\")\n",
    "\n",
    "imgs=list(map(CIFAR2img, imgs))\n",
    "img_df = pd.DataFrame(imgs, columns=['imgs'])\n",
    "\n",
    "\n",
    "\n",
    "# DEVICE = torch.device('cpu')\n",
    "# pipe = pipeline(task=\"image-feature-extraction\", model_name=\"google/vit-base-patch16-384\", device=DEVICE, pool=True)\n",
    "\n",
    "# output = pipe(imgs, return_tensors=True)\n",
    "# img_embeddings=torch.stack(output).squeeze()\n",
    "\n",
    "X_mod_tild = np.array([txt_tild_df, tab_tild_df, img_tild_df])\n",
    "N=min([np.shape(X)[0] for X in X_mod_tild])\n",
    "X_mod_tild = np.array([txt_tild_df[0:N], tab_tild_df[0:N], img_tild_df[0:N]])\n",
    "\n",
    "\n",
    "#contruct synthetic data\n",
    "theta0=0.5\n",
    "# X_mod_tild=[[log(price)],[img label],[txt sentiment]] np_arr\n",
    "g0_tild=np.sum(np.array(list(map(lambda X: (X - np.full(N,np.mean(X)))/np.std(X) ,X_mod_tild))),axis=0)\n",
    "\n",
    "m0_tild= -g0_tild\n",
    "\n",
    "np.random.seed(20)\n",
    "D=m0_tild + np.random.normal(0,1,size=N)\n",
    "Y=theta0*D + g0_tild + np.random.normal(0,1,size=N)\n",
    "\n",
    "#append D,Y onto df X\n",
    "X_mod=tab_df[:N].join(other=[txt_df[:N], img_df[:N]])\n",
    "#NOTE: need to double check with full 10000/50000 batches \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#output DataFrame X with cols [carat,cut,color,clarity,depth,table,x,y,z  ,  review_text   ,   img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google/vit-base-patch16-224 and revision 3f49326 (https://huggingface.co/google/vit-base-patch16-224).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([10, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#output=torch.tensor(output).squeeze()\n",
    "\n",
    "\n",
    "#runtime with 10 batch = 1m43 on slow img processor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.21035791 -1.16587913 -1.16076029 ... -1.49003131 -1.49003131\n",
      "  1.90263842]\n"
     ]
    }
   ],
   "source": [
    "#contruct synthetic data\n",
    "theta0=0.5\n",
    "# X_mod_tild=[[log(price)],[img label],[txt sentiment]] np_arr\n",
    "g0_tild=np.sum(np.array(list(map(lambda X: (X - np.full(N,np.mean(X)))/np.std(X) ,X_mod_tild))),axis=0)\n",
    "print(g0_tild)\n",
    "m0_tild= -g0_tild\n",
    "\n",
    "np.random.seed(20)\n",
    "D=m0_tild + np.random.normal(0,1,size=N)\n",
    "Y=theta0*D + g0_tild + np.random.normal(0,1,size=N)\n",
    "\n",
    "#append D,Y onto df X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up architecture/model: Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deep/embedding common classes\n",
    "\n",
    "\n",
    "# embedding_extractor = pipeline(model=\"google-bert/bert-base-cased\", task=\"feature-extraction\", device=0)\n",
    "# result = embedding_extractor(\"This is a simple test.\", return_tensors=True)\n",
    "\n",
    "txt_model = \"FacebookAI/roberta-base\"\n",
    "txt_embedding_extractor = pipeline(model=txt_model, task=\"feature-extraction\", device=0 if torch.cuda.is_available() else -1)\n",
    "txt_embeddings= txt_embedding_extractor(all_text_tensor, return_tensors=True)\n",
    "\n",
    "img_model=\"VIT\"\n",
    "\n",
    "tab_model = \"SAINT\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MMEmbeddingNetwork(nn.Module):\n",
    "    def __init__(self, img_embed_size, txt_embed_size,HE_size,hiddenSize=100):\n",
    "        super(self).__init__()\n",
    "        # self.inp = nn.Linear(imgEmbSize+txtEmbSize, hiddenSize)\n",
    "        # self.fc = nn.Linear(hiddenSize,hiddenSize)\n",
    "        # self.out = nn.Linear(hiddenSize,H_ESize)\n",
    "        self.fc=nn.Linear(img_embed_size+txt_embed_size, HE_size)\n",
    "    def forward(self,comb_embed):\n",
    "        # x=combinedEmbed\n",
    "        # x=self.inp(x)\n",
    "        # x=self.fc(x)\n",
    "        # return self.out(x)\n",
    "        x=self.fc(comb_embed)\n",
    "        return activF(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up architecture/model: Deep\n",
    "    \n",
    "\n",
    "class PredictiveUnit(nn.Module):\n",
    "    def __init__(self,gen_embed_size):\n",
    "        super(self).__init__()\n",
    "        self.fhead_outcome = nn.Linear(gen_embed_size,1)\n",
    "        self.fhead_treat = nn.Linear(gen_embed_size,1)\n",
    "    \n",
    "    def forward(self, gen_embed):\n",
    "        l_hat=self.fhead_outcome(gen_embed)\n",
    "        m_hat=self.fhead_treat(gen_embed)\n",
    "        return m_hat,l_hat\n",
    "    \n",
    "\n",
    "class DeepModel(nn.Module):\n",
    "    def __init__(self, txt_pipeline, img_pipeline, tab_pipeline, img_embed_size, txt_embed_size,HE_size,gen_embed_size):\n",
    "        super(self).__init__()\n",
    "        self.txt_in,self.img_in,self.tab_in=txt_pipeline, img_pipeline, tab_pipeline\n",
    "        self.multimod = MMEmbeddingNetwork(img_embed_size, txt_embed_size,HE_size)\n",
    "        self.pred=PredictiveUnit(gen_embed_size)\n",
    "\n",
    "    def forward(self, txt, img, tab):\n",
    "        txt_embed=self.txt_in(txt)\n",
    "        img_embed = self.img_in(img)\n",
    "        tab_embed = self.tab_in(tab)\n",
    "        comb_embed = txt_embed + img_embed \n",
    "        H_E = self.multimod(comb_embed)\n",
    "        G_E = H_E + tab_embed\n",
    "        return self.pred(G_E)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up architecture/model: Embedding\n",
    "\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, boost_alg,txt_pipeline, img_pipeline, img_embed_size, txt_embed_size,HE_size):\n",
    "        self.txt_in,self.img_in=txt_pipeline, img_pipeline\n",
    "        self.multimod = MMEmbeddingNetwork(img_embed_size, txt_embed_size,HE_size)\n",
    "        self.boosting_alg= boost_alg\n",
    "    def forward(self, txt, img, tab):\n",
    "        txt_embed=self.txt_in(txt)\n",
    "        img_embed = self.img_in(img)\n",
    "        comb_embed = txt_embed + img_embed \n",
    "        H_E = self.multimod(comb_embed)\n",
    "        return self.boosting_alg(H_E, tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML training loop: Deep\n",
    "def DeepLoss(D,Y,m_hat,l_hat):\n",
    "    \"\"\"can handle 1-D vectors\"\"\"\n",
    "    D_rms_err=torch.sqrt(torch.sum(torch.square(D-m_hat)))\n",
    "    Y_rms_err=torch.sqrt(torch.sum(torch.square(Y-l_hat)))\n",
    "    return D_rms_err*Y_rms_err\n",
    "\n",
    "deepnet=DeepModel(txt_pipeline, img_pipeline, tab_pipeline, img_embed_size, txt_embed_size,HE_size,gen_embed_size)\n",
    "\n",
    "def train(deepnet,n_epochs=1000, batch_size=100,loss_fn=DeepLoss):\n",
    "    \"\"\"Training params need work\n",
    "    form depends on pd.DataFrame/torch.tensor implementation details\"\"\"\n",
    "    optimizer=torch.optim.Adam(deepnet.parameters(),lr=0.001)\n",
    "    for i_epoch in tqdm(range(n_epochs)):\n",
    "        \n",
    "        #NOTE: it's about to get spicy here!\n",
    "        \n",
    "        m_hat,l_hat = deepnet(txt, img, tab)\n",
    "        loss = loss_fn(D,Y,m_hat,l_hat)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Finished {i_epoch+1}/n_epoch, loss = {loss}\", end = '\\r')\n",
    "\n",
    "train(deepnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML training loop: Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common DoubleML pass-through implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save and export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import model (if needed (how to structure selective cell runs?))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting/performance analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
