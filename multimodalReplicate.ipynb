{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper this is based attempting to replicate: https://arxiv.org/pdf/2402.01785\n",
    "\n",
    "Useful DoubleML docs: https://docs.doubleml.org/stable/guide/guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "import doubleml\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import warnings\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import clone\n",
    "\n",
    "from doubleml import DoubleMLData\n",
    "from doubleml import DoubleMLPLR\n",
    "from doubleml.datasets import make_plr_CCDDHNR2018\n",
    "\n",
    "face_colors = sns.color_palette('pastel')\n",
    "edge_colors = sns.color_palette('dark')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"init complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models \n",
    "#### Baseline Model\n",
    "Uses `LightGBM` package _only_ to estimate nuisance elements on _only_ the tabular data \n",
    "\n",
    "#### Deep Model\n",
    "Implemented exactly as in _Figure 2_ in paper\n",
    "- For text, they use a `RoBERTa` Model pretrained on a `Twitter` Dataset\n",
    "- For images, they use a `VIT` Model pretrained on the `ImageNet-21k` Dataset\n",
    "- For tabular data, they use a `SAINT` model implemented in `pytorch-widedeep`\n",
    "\n",
    "#### Embedding Model \n",
    "Builds on the `Deep Model`, but instead of passing embeddings directly through fusion head/predictive workflow, passes general embedding $H_e$ and data $X_{tab}$ to a boosting algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets (change labelling to be \\textbf)\n",
    "### Tabular\n",
    "`DIAMONDS` dataset, downsampled to create dataset with $N=50,000$ \n",
    "- $\\tilde{X}_{tab}$ is the logarithm of the price\n",
    "- $X_{tab}$ is everything else\n",
    "### Image\n",
    "`CIFAR-10` dataset, specifically the training set ($N=50,000$), which is 32x32 colour images in 10 different classes\n",
    " - $\\tilde{X}_{img}$ is a numerical representation of the label\n",
    " - $X_{img}$ is the image itself\n",
    "### Text\n",
    "`IMDB` dataset, both the training and test samples\n",
    "- $\\tilde{X}_{txt}$ is the binary (positive/negative) sentiment label\n",
    "- $X_{txt}$ is the review itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process real data\n",
    "tab_df = pd.read_csv(\"diamonds.csv\")\n",
    "\n",
    "cut_di = {'Ideal':5, 'Premium':4, 'Good':2, 'Very Good':3, 'Fair':1}\n",
    "tab_df['cut'].replace(cut_di,inplace=True)\n",
    "col_di={'E': 2, 'I': 6, 'J': 7, 'H': 5, 'F': 3, 'G': 4, 'D': 1}\n",
    "tab_df['color'].replace(col_di,inplace=True)\n",
    "clar_di={'SI2': 1, 'SI1': 2, 'VS1': 4, 'VS2': 3, 'VVS2': 5, 'VVS1': 6, 'I1': 0, 'IF': 7}\n",
    "tab_df['clarity'].replace(clar_di,inplace=True)\n",
    "\n",
    "tab_tild_df=np.log(tab_df['price'])\n",
    "tab_df=tab_df.drop(columns=['price','Unnamed: 0'])\n",
    "\n",
    "\n",
    "\n",
    "txt_df=pd.read_csv(\"IMDB Dataset.csv\")\n",
    "\n",
    "sent_di={'positive':1,'negative':0}\n",
    "txt_df['sentiment'].replace(sent_di,inplace=True)\n",
    "txt_tild_df = txt_df['sentiment']\n",
    "txt_df=txt_df['review']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "img_df=False\n",
    "\n",
    "#need to extract from pd to np\n",
    "X_mod_tild = np.array([txt_tild_df, tab_tild_df, img_tild_df])\n",
    "\n",
    "#output DataFrame X with cols [carat,cut,color,clarity,depth,table,x,y,z  ,  review_text   ,   img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file,'rb') as fo:\n",
    "        dict=pickle.load(fo,encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "img_dict=unpickle(r'cifar-10-batches-py\\data_batch_1')\n",
    "labels=img_dict[b'labels']\n",
    "names=img_dict[b'names']\n",
    "imgs=img_dict[b'filenames']\n",
    "print(img_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contruct synthetic data\n",
    "N=50000\n",
    "theta0=0.5\n",
    "# X_mod_tild=[[log(price)],[img label],[txt sentiment]] np_arr\n",
    "g0_tild=np.sum(np.array(map(lambda X: (X - np.full(N,np.mean(X)))/np.std(X) ,X_mod_tild)),axis=0)\n",
    "m0_tild= -g0_tild\n",
    "\n",
    "np.random.seed(20)\n",
    "D=m0_tild + np.random.normal(0,1,size=N)\n",
    "Y=theta0*D + g0_tild + np.random.normal(0,1,size=N)\n",
    "\n",
    "#append D,Y onto df X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up architecture/model: Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deep/embedding common classes\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# embedding_extractor = pipeline(model=\"google-bert/bert-base-cased\", task=\"feature-extraction\", device=0)\n",
    "# result = embedding_extractor(\"This is a simple test.\", return_tensors=True)\n",
    "\n",
    "txt_model = \"ROBERTA\"\n",
    "txt_embedding_extractor = pipeline(model=\"google-bert/bert-base-cased\", task=\"feature-extraction\", device=0 if torch.cuda.is_available() else -1)\n",
    "txt_embeddings= txt_embedding_extractor(all_text_tensor, return_tensors=True)\n",
    "\n",
    "img_model=\"VIT\"\n",
    "\n",
    "tab_model = \"SAINT\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MMEmbeddingNetwork(nn.Module):\n",
    "    def __init__(self, img_embed_size, txt_embed_size,HE_size,hiddenSize=100):\n",
    "        super(self).__init__()\n",
    "        # self.inp = nn.Linear(imgEmbSize+txtEmbSize, hiddenSize)\n",
    "        # self.fc = nn.Linear(hiddenSize,hiddenSize)\n",
    "        # self.out = nn.Linear(hiddenSize,H_ESize)\n",
    "        self.fc=nn.Linear(img_embed_size+txt_embed_size, HE_size)\n",
    "    def forward(self,comb_embed):\n",
    "        # x=combinedEmbed\n",
    "        # x=self.inp(x)\n",
    "        # x=self.fc(x)\n",
    "        # return self.out(x)\n",
    "        x=self.fc(comb_embed)\n",
    "        return activF(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up architecture/model: Deep\n",
    "    \n",
    "\n",
    "class PredictiveUnit(nn.Module):\n",
    "    def __init__(self,gen_embed_size):\n",
    "        super(self).__init__()\n",
    "        self.fhead_outcome = nn.Linear(gen_embed_size,1)\n",
    "        self.fhead_treat = nn.Linear(gen_embed_size,1)\n",
    "    \n",
    "    def forward(self, gen_embed):\n",
    "        l_hat=self.fhead_outcome(gen_embed)\n",
    "        m_hat=self.fhead_treat(gen_embed)\n",
    "        return m_hat,l_hat\n",
    "    \n",
    "\n",
    "class DeepModel(nn.Module):\n",
    "    def __init__(self, txt_pipeline, img_pipeline, tab_pipeline, img_embed_size, txt_embed_size,HE_size,gen_embed_size):\n",
    "        super(self).__init__()\n",
    "        self.txt_in,self.img_in,self.tab_in=txt_pipeline, img_pipeline, tab_pipeline\n",
    "        self.multimod = MMEmbeddingNetwork(img_embed_size, txt_embed_size,HE_size)\n",
    "        self.pred=PredictiveUnit(gen_embed_size)\n",
    "\n",
    "    def forward(self, txt, img, tab):\n",
    "        txt_embed=self.txt_in(txt)\n",
    "        img_embed = self.img_in(img)\n",
    "        tab_embed = self.tab_in(tab)\n",
    "        comb_embed = txt_embed + img_embed \n",
    "        H_E = self.multimod(comb_embed)\n",
    "        G_E = H_E + tab_embed\n",
    "        return self.pred(G_E)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up architecture/model: Embedding\n",
    "\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, boost_alg,txt_pipeline, img_pipeline, img_embed_size, txt_embed_size,HE_size):\n",
    "        self.txt_in,self.img_in=txt_pipeline, img_pipeline\n",
    "        self.multimod = MMEmbeddingNetwork(img_embed_size, txt_embed_size,HE_size)\n",
    "        self.boosting_alg= boost_alg\n",
    "    def forward(self, txt, img, tab):\n",
    "        txt_embed=self.txt_in(txt)\n",
    "        img_embed = self.img_in(img)\n",
    "        comb_embed = txt_embed + img_embed \n",
    "        H_E = self.multimod(comb_embed)\n",
    "        return self.boosting_alg(H_E, tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML training loop: Deep\n",
    "def DeepLoss(D,Y,m_hat,l_hat):\n",
    "    \"\"\"can handle 1-D vectors\"\"\"\n",
    "    D_rms_err=torch.sqrt(torch.sum(torch.square(D-m_hat)))\n",
    "    Y_rms_err=torch.sqrt(torch.sum(torch.square(Y-l_hat)))\n",
    "    return D_rms_err*Y_rms_err\n",
    "\n",
    "deepnet=DeepModel(txt_pipeline, img_pipeline, tab_pipeline, img_embed_size, txt_embed_size,HE_size,gen_embed_size)\n",
    "\n",
    "def train(deepnet,n_epochs=1000, batch_size=100,loss_fn=DeepLoss):\n",
    "    \"\"\"Training params need work\n",
    "    form depends on pd.DataFrame/torch.tensor implementation details\"\"\"\n",
    "    optimizer=torch.optim.Adam(deepnet.parameters(),lr=0.001)\n",
    "    for i_epoch in tqdm(range(n_epochs)):\n",
    "        \n",
    "        #NOTE: it's about to get spicy here!\n",
    "        \n",
    "        m_hat,l_hat = deepnet(txt, img, tab)\n",
    "        loss = loss_fn(D,Y,m_hat,l_hat)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Finished {i_epoch+1}/n_epoch, loss = {loss}\", end = '\\r')\n",
    "\n",
    "train(deepnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML training loop: Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common DoubleML pass-through implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save and export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import model (if needed (how to structure selective cell runs?))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting/performance analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
